# .github/workflows/deploy-llmops.yml
name: Deploy LLMOps Endpoint (100% Free)

on:
  push:
    branches: [ lab1 ]
  workflow_dispatch:
    inputs:
        workflow-path:
          description: 'Path to the workflow file'
          required: true
          default: '.github\workflows\lab1-workflow.yml'
jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      # 1. Checkout code
      - name: Checkout
        uses: actions/checkout@v4

      # 2. Set up Docker Buildx
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # 3. Login to GitHub Container Registry (ghcr.io)
      - name: Login to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # 4. Set lowercase repository name
      - name: Set lowercase repo name
        run: |
          echo "REPO_LOWER=$(echo ${{ github.repository }} | tr '[:upper:]' '[:lower:]')" >> $GITHUB_ENV

      # 5. Build and push Docker image
      - name: Build and push
        uses: docker/build-push-action@v6
        with:
          context: llmops/Lab1-LLMOps-Pipeline/app
          push: true
          tags: ghcr.io/${{ env.REPO_LOWER }}/llmops-api:latest

      # 6. Install kind (Kubernetes IN Docker) - 100% FREE local cluster
      - name: Create Kind Cluster
        uses: helm/kind-action@v1.10.0
        with:
          cluster_name: llmops-cluster
          version: v0.23.0

      # 7. Substitute variables in deployment.yaml
      - name: Prepare Kubernetes manifests
        run: |
          # Install yq for YAML manipulation
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          
          # Split the multi-document YAML file
          cd llmops/Lab1-LLMOps-Pipeline/k8s
          yq eval 'select(di == 0)' deployment.yaml > deployment-only.yaml
          yq eval 'select(di == 1)' deployment.yaml > service-only.yaml
          
          # Update the image name in the Deployment only
          export IMAGE_NAME="ghcr.io/${{ env.REPO_LOWER }}/llmops-api:latest"
          yq eval ".spec.template.spec.containers[0].image = strenv(IMAGE_NAME)" -i deployment-only.yaml
          
          # Recombine the files
          cat deployment-only.yaml > deployment.yaml
          echo "---" >> deployment.yaml
          cat service-only.yaml >> deployment.yaml
          
          # Cleanup temp files
          rm deployment-only.yaml service-only.yaml
          
          cd ../../..
          echo "=== Updated deployment.yaml ==="
          cat llmops/Lab1-LLMOps-Pipeline/k8s/deployment.yaml
          echo ""
          echo "=== Verifying image name ==="
          yq eval 'select(di == 0) | .spec.template.spec.containers[0].image' llmops/Lab1-LLMOps-Pipeline/k8s/deployment.yaml
          echo "=== Verifying Service is intact ==="
          yq eval 'select(di == 1) | .kind' llmops/Lab1-LLMOps-Pipeline/k8s/deployment.yaml

      # 8. Create image pull secret for GHCR
      - name: Create image pull secret
        run: |
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=ghcr.io \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GITHUB_TOKEN }} \
            --docker-email=${{ github.actor }}@users.noreply.github.com || true

      # 8b. Create secret for API keys (OpenAI, Azure, etc.)
      - name: Create API key secrets
        run: |
          echo "=== Checking GitHub Secrets (masked) ==="
          echo "OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY != '' && 'SET' || 'NOT SET' }}"
          echo "AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY != '' && 'SET' || 'NOT SET' }}"
          echo "AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT != '' && 'SET' || 'NOT SET' }}"
          echo "GROQ_API_KEY: ${{ secrets.GROQ_API_KEY != '' && 'SET' || 'NOT SET' }}"
          
          kubectl create secret generic llmops-api-keys \
            --from-literal=OPENAI_API_KEY="${{ secrets.OPENAI_API_KEY }}" \
            --from-literal=AZURE_OPENAI_KEY="${{ secrets.AZURE_OPENAI_API_KEY }}" \
            --from-literal=AZURE_OPENAI_ENDPOINT="${{ secrets.AZURE_OPENAI_ENDPOINT }}" \
            --from-literal=GROQ_API_KEY="${{ secrets.GROQ_API_KEY }}" \
            --dry-run=client -o yaml | kubectl apply -f -
          echo "API secrets created/updated"
          
          echo "=== Verifying Kubernetes Secret ==="
          kubectl get secret llmops-api-keys -o jsonpath='{.data}' | jq 'keys'


      # 9. Deploy Redis first
      - name: Deploy Redis
        run: |
          kubectl apply -f llmops/Lab1-LLMOps-Pipeline/k8s/redis.yaml
          kubectl rollout status deployment/redis -n default --timeout=120s
          echo "Redis is ready"

      # 10. Deploy API to Kind (real K8s, no cloud cost)
      - name: Deploy to Kubernetes
        run: |
          kubectl apply -f llmops/Lab1-LLMOps-Pipeline/k8s/deployment.yaml
          kubectl rollout status deployment/llmops-api -n default --timeout=300s
      
      # 10b. Verify environment variables in pod
      - name: Verify Pod Environment Variables
        run: |
          echo "=== Checking Pod Environment Variables ==="
          POD_NAME=$(kubectl get pods -l app=llmops-api -o jsonpath='{.items[0].metadata.name}')
          echo "Pod: $POD_NAME"
          
          echo "Checking if API keys are set (values masked):"
          kubectl exec $POD_NAME -- sh -c 'echo "OPENAI_API_KEY: ${OPENAI_API_KEY:+SET}" || echo "OPENAI_API_KEY: NOT SET"'
          kubectl exec $POD_NAME -- sh -c 'echo "AZURE_OPENAI_KEY: ${AZURE_OPENAI_KEY:+SET}" || echo "AZURE_OPENAI_KEY: NOT SET"'
          kubectl exec $POD_NAME -- sh -c 'echo "AZURE_OPENAI_ENDPOINT: ${AZURE_OPENAI_ENDPOINT}" -- || echo "AZURE_OPENAI_ENDPOINT: NOT SET"'
          kubectl exec $POD_NAME -- sh -c 'echo "AZURE_OPENAI_ENDPOINT LENGTH: ${#AZURE_OPENAI_ENDPOINT}" '

          echo "=== Pod Startup Logs ==="
          kubectl logs $POD_NAME --tail=300 | grep -E "(LLM|MLflow|OpenTelemetry|Provider|AZURE)" || true
      
      # 9. Debug deployment if it fails
      - name: Debug deployment on failure
        if: failure()
        run: |
          echo "=== Deployment Status ==="
          kubectl get deployments
          echo "=== Pod Status ==="
          kubectl get pods -o wide
          echo "=== Pod Descriptions ==="
          kubectl describe pods -l app=llmops-api
          echo "=== Pod Logs ==="
          kubectl logs -l app=llmops-api --tail=100 || true

      # 11. Port-forward for testing
      - name: Expose service
        run: |
          kubectl port-forward svc/llmops-service 8000:8000 --address 0.0.0.0 &
          sleep 5
          echo "API running at http://localhost:8000"

      # 12. Run integration tests
      - name: Test endpoint
        id: run_tests
        continue-on-error: true
        run: |
          pip install requests pytest
          cd llmops/Lab1-LLMOps-Pipeline
          pytest tests/test_api.py -v -s
      
      # 12b. Always show detailed logs for debugging
      - name: Show API logs and debug info
        if: always()
        run: |
          echo "=== API Pod Logs (last 100 lines) ==="
          POD_NAME=$(kubectl get pods -l app=llmops-api -o jsonpath='{.items[0].metadata.name}')
          kubectl logs $POD_NAME --tail=100
          
          echo ""
          echo "=== Testing API directly ==="
          curl -X POST http://localhost:8000/v1/chat/completions \
            -H "Content-Type: application/json" \
            -d '{"messages": [{"role": "user", "content": "test"}], "metadata": {"department": "test"}}' \
            -v || true
          
          echo ""
          echo "=== Pod Environment Check ==="
          kubectl exec $POD_NAME -- sh -c 'echo "AZURE_OPENAI_KEY length: ${#AZURE_OPENAI_KEY}"'
          kubectl exec $POD_NAME -- sh -c 'echo "AZURE_OPENAI_ENDPOINT: ${#AZURE_OPENAI_ENDPOINT}"'
      
      # 12c. Fail the workflow if tests failed
      - name: Check test results
        if: steps.run_tests.outcome == 'failure'
        run: exit 1

      # 13. Run Ragas Evaluations (Separate Job)
      - name: Run Ragas Evaluations
        if: steps.run_tests.outcome == 'success'
        run: |
          echo "=== Deploying Evaluation Job ==="
          # Delete previous job if it exists
          kubectl delete job llmops-eval-job --ignore-not-found=true
          kubectl delete configmap eval-scripts --ignore-not-found=true
          
          # Create and run the new job
          kubectl apply -f llmops/Lab1-LLMOps-Pipeline/k8s/eval-job.yaml
          
          echo "Waiting for evaluation job to complete..."
          kubectl wait --for=condition=complete job/llmops-eval-job --timeout=300s
          
          echo "=== Evaluation Logs ==="
          kubectl logs job/llmops-eval-job

      # 10. Optional: Deploy to Oracle Cloud Free Tier OKE (uncomment if you have OCI)
      # - name: Deploy to Oracle OKE (Free Forever)
      #   if: false
      #   uses: oracle-actions/setup-oci-cli@v1
      #   with:
      #     oci-config: ${{ secrets.OCI_CONFIG }}

      # 11. Success message
      - name: Success
        run: |
          echo "LLMOps API deployed successfully!"
          echo "Access at: http://localhost:8080/v1/chat/completions"
          echo "Live in GitHub Codespaces: https://github.com/codespaces"